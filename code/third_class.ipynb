{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO1h3yILNVViLTCIrpdq6kb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7x9tYoPkc0zq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736497281735,"user_tz":-540,"elapsed":20373,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"9fbfa877-f249-448e-96e7-436425ad6936"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys, torch, os, json, copy\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from collections import Counter\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/hemp/download/model')\n","from quant import QuantizableMobileNetV4\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch import nn, Tensor\n","from mobilenet import mobilenetv4_conv_medium\n","import torch.nn.utils.prune as prune"],"metadata":{"id":"NWb2yHXftsgF","executionInfo":{"status":"ok","timestamp":1736497303326,"user_tz":-540,"elapsed":21593,"user":{"displayName":"전주석","userId":"05296464826010259461"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# **데이터셋 정의**"],"metadata":{"id":"2DbJl1se2Ls1"}},{"cell_type":"code","source":["input_dir = '/content/drive/MyDrive/Colab Notebooks/hemp/download/dataset/seg_images'\n","label_dir = '/content/drive/MyDrive/Colab Notebooks/hemp/download/dataset/labels'\n","\n","def load_dataset(input_dir, label_dir):\n","    intput_files = os.listdir(input_dir)\n","    label_files = os.listdir(label_dir)\n","\n","    dataset = []\n","    for input_file in intput_files:\n","        label_file = input_file.replace('.png', '.json')\n","        with open(os.path.join(label_dir, label_file), 'r') as f:\n","            label_data = json.load(f)\n","            browning = label_data['annotations']['polygon'][0]['browning']\n","\n","        dataset.append({\n","            'image_path': os.path.join(input_dir, input_file),\n","            'label': f'{browning}'\n","        })\n","    return dataset\n","\n","dataset = load_dataset(input_dir, label_dir)\n","labels = [data['label'] for data in dataset]\n","\n","train_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n","for train_idx, temp_idx in train_split.split(dataset, labels):\n","    train_set = [dataset[i] for i in train_idx]\n","    temp_set = [dataset[i] for i in temp_idx]\n","val_test_split = StratifiedShuffleSplit(n_splits=1, test_size=2/3, random_state=42)\n","labels_temp = [labels[i] for i in temp_idx]\n","for val_idx, test_idx in val_test_split.split(temp_set, labels_temp):\n","    val_set = [temp_set[i] for i in val_idx]\n","    test_set = [temp_set[i] for i in test_idx]\n","\n","train_label_count = Counter([data['label'] for data in train_set])\n","val_label_count   = Counter([data['label'] for data in val_set])\n","test_label_count  = Counter([data['label'] for data in test_set])\n","\n","print(\"Train set class counts:\", train_label_count)\n","print(\"Validation set class counts:\", val_label_count)\n","print(\"Test set class counts:\", test_label_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2btLvgg72CbU","executionInfo":{"status":"ok","timestamp":1736497306329,"user_tz":-540,"elapsed":3005,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"06a23640-05bb-4fe8-a15e-d247e3d89942"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set class counts: Counter({'True': 35, 'False': 35})\n","Validation set class counts: Counter({'False': 5, 'True': 5})\n","Test set class counts: Counter({'False': 10, 'True': 10})\n"]}]},{"cell_type":"code","source":["MEAN = [0.485, 0.456, 0.406]\n","STD = [0.229, 0.224, 0.225]\n","normalize = transforms.Normalize(mean=MEAN, std=STD)\n","transform = transforms.Compose([\n","    transforms.ToTensor()])\n","\n","def preprocess_image(image):\n","    non_zero_mask = (image > 0).any(axis=-1)\n","    non_zero_rows = np.any(non_zero_mask, axis=1)\n","    non_zero_cols = np.any(non_zero_mask, axis=0)\n","    min_row, max_row = np.where(non_zero_rows)[0][[0, -1]]\n","    min_col, max_col = np.where(non_zero_cols)[0][[0, -1]]\n","    image = image[min_row:max_row+1, min_col:max_col+1]\n","    image = Image.fromarray(image)\n","    image = image.resize((224, 224))\n","    return image\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None, num_classes=2):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","        self.num_classes = num_classes\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n","        image = preprocess_image(image)\n","        label = int(self.labels[idx])\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","def create_dataloader(dataset, transform, batch_size, shuffle, num_workers):\n","    image_paths = [item['image_path'] for item in dataset]\n","    labels = [0 if item['label'] == 'True' else 1 for item in dataset]\n","    custom_dataset = CustomDataset(image_paths, labels, transform=transform)\n","    return DataLoader(custom_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","\n","train_loader = create_dataloader(train_set, transform, batch_size=24, shuffle=True, num_workers=2)\n","val_loader = create_dataloader(val_set, transform, batch_size=24, shuffle=False, num_workers=2)\n","test_loader = create_dataloader(test_set, transform, batch_size=24, shuffle=False, num_workers=2)"],"metadata":{"id":"uifGGI0F2KDD","executionInfo":{"status":"ok","timestamp":1736497306329,"user_tz":-540,"elapsed":2,"user":{"displayName":"전주석","userId":"05296464826010259461"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **FP32**"],"metadata":{"id":"TRXs-Z3I5J0i"}},{"cell_type":"code","source":["def print_model_size(mdl):\n","    torch.save(mdl.state_dict(), \"tmp.pt\")\n","    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n","    os.remove('tmp.pt')\n","\n","cpu_device = torch.device(\"cpu\")\n","CE = nn.CrossEntropyLoss()\n","\n","pretrained_path = '/content/drive/MyDrive/Colab Notebooks/hemp/download/model/pretrain.pth'\n","FP32 = mobilenetv4_conv_medium(num_classes=2)\n","FP32.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","FP32.eval()\n","\n","total_loss = 0.0\n","correct_predictions = 0\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc='Test Progress', leave=False):\n","        inputs = normalize(inputs).to(cpu_device)\n","        labels = labels.to(cpu_device)\n","\n","        outputs = FP32(inputs)\n","        loss = CE(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","        total_loss += loss.item() * inputs.size(0)\n","        correct_predictions += (preds == labels).sum().item()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(preds.cpu().numpy())\n","\n","avg_test_loss = total_loss / len(test_loader.dataset)\n","avg_test_acc = accuracy_score(all_labels, all_preds)\n","test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","print(f\"Test Metrics: \"\n","      f\"Loss={avg_test_loss:.4f}, \"\n","      f\"Acc={avg_test_acc:.4f}, \"\n","      f\"Precision={test_precision:.4f}, \"\n","      f\"Recall={test_recall:.4f}, \"\n","      f\"F1={test_f1:.4f}\")\n","\n","print_model_size(FP32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvUwdLD_5N6q","executionInfo":{"status":"ok","timestamp":1736497332590,"user_tz":-540,"elapsed":26263,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"497048bb-0e77-425b-ce37-d94cc7e7a193"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-2af946d792ad>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  FP32.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","                                                            "]},{"output_type":"stream","name":"stdout","text":["Test Metrics: Loss=0.1260, Acc=0.9500, Precision=0.9545, Recall=0.9500, F1=0.9499\n","34.15 MB\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"markdown","source":["# **비구조적 가지치기 (Unstructured pruning)**"],"metadata":{"id":"zha4k5PR5_vN"}},{"cell_type":"code","source":["model = QuantizableMobileNetV4(num_classes=2)\n","model.to(cpu_device)\n","\n","model.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","\n","model.eval()\n","model.fuse_model()\n","model.train()\n","\n","model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n","torch.ao.quantization.prepare_qat(model, inplace=True)\n","\n","ori_model = copy.deepcopy(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoMF0xNF6D2V","executionInfo":{"status":"ok","timestamp":1736497333469,"user_tz":-540,"elapsed":885,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"6b170bb4-3151-4b15-9458-3ff40e94ab9e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-12aa41557af0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["prunned_model = copy.deepcopy(ori_model)\n","\n","for name, module in prunned_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        prune.l1_unstructured(module, name='weight', amount=0.3)\n","\n","def count_sparsity(model):\n","    total_params = 0\n","    total_zero = 0\n","    for name, module in model.named_modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            weight_sparsity = float(torch.sum(module.weight == 0)) / module.weight.numel()\n","            print(f\"{name}: weight sparsity={weight_sparsity*100:.2f}%\")\n","            total_zero += torch.sum(module.weight == 0).item()\n","            total_params += module.weight.numel()\n","    print(f\"Total zeros: {total_zero} out of {total_params} ({(total_zero / total_params) * 100:.2f}%)\")\n","\n","count_sparsity(prunned_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Daotx5r08SqQ","executionInfo":{"status":"ok","timestamp":1736497334219,"user_tz":-540,"elapsed":753,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"e5dafae0-926e-4c89-99d5-f6f68266d067"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["features.0.block.0: weight sparsity=29.98%\n","features.1.block.0: weight sparsity=30.00%\n","features.2.block.0: weight sparsity=30.00%\n","features.3.start_dw_conv: weight sparsity=30.09%\n","features.3.expand_conv: weight sparsity=30.00%\n","features.3.middle_dw_conv: weight sparsity=30.00%\n","features.3.proj_conv: weight sparsity=30.00%\n","features.4.start_dw_conv: weight sparsity=30.00%\n","features.4.expand_conv: weight sparsity=30.00%\n","features.4.middle_dw_conv: weight sparsity=30.00%\n","features.4.proj_conv: weight sparsity=30.00%\n","features.5.start_dw_conv: weight sparsity=30.00%\n","features.5.expand_conv: weight sparsity=30.00%\n","features.5.middle_dw_conv: weight sparsity=30.00%\n","features.5.proj_conv: weight sparsity=30.00%\n","features.6.start_dw_conv: weight sparsity=30.00%\n","features.6.expand_conv: weight sparsity=30.00%\n","features.6.middle_dw_conv: weight sparsity=30.00%\n","features.6.proj_conv: weight sparsity=30.00%\n","features.7.start_dw_conv: weight sparsity=30.00%\n","features.7.expand_conv: weight sparsity=30.00%\n","features.7.middle_dw_conv: weight sparsity=30.00%\n","features.7.proj_conv: weight sparsity=30.00%\n","features.8.start_dw_conv: weight sparsity=30.00%\n","features.8.expand_conv: weight sparsity=30.00%\n","features.8.middle_dw_conv: weight sparsity=30.00%\n","features.8.proj_conv: weight sparsity=30.00%\n","features.9.start_dw_conv: weight sparsity=30.00%\n","features.9.expand_conv: weight sparsity=30.00%\n","features.9.middle_dw_conv: weight sparsity=30.00%\n","features.9.proj_conv: weight sparsity=30.00%\n","features.10.start_dw_conv: weight sparsity=30.00%\n","features.10.expand_conv: weight sparsity=30.00%\n","features.10.proj_conv: weight sparsity=30.00%\n","features.11.expand_conv: weight sparsity=30.00%\n","features.11.proj_conv: weight sparsity=30.00%\n","features.12.start_dw_conv: weight sparsity=30.00%\n","features.12.expand_conv: weight sparsity=30.00%\n","features.12.proj_conv: weight sparsity=30.00%\n","features.13.start_dw_conv: weight sparsity=30.00%\n","features.13.expand_conv: weight sparsity=30.00%\n","features.13.middle_dw_conv: weight sparsity=30.00%\n","features.13.proj_conv: weight sparsity=30.00%\n","features.14.start_dw_conv: weight sparsity=30.00%\n","features.14.expand_conv: weight sparsity=30.00%\n","features.14.middle_dw_conv: weight sparsity=30.00%\n","features.14.proj_conv: weight sparsity=30.00%\n","features.15.start_dw_conv: weight sparsity=29.99%\n","features.15.expand_conv: weight sparsity=30.00%\n","features.15.middle_dw_conv: weight sparsity=30.00%\n","features.15.proj_conv: weight sparsity=30.00%\n","features.16.start_dw_conv: weight sparsity=29.99%\n","features.16.expand_conv: weight sparsity=30.00%\n","features.16.middle_dw_conv: weight sparsity=30.00%\n","features.16.proj_conv: weight sparsity=30.00%\n","features.17.expand_conv: weight sparsity=30.00%\n","features.17.proj_conv: weight sparsity=30.00%\n","features.18.start_dw_conv: weight sparsity=29.99%\n","features.18.expand_conv: weight sparsity=30.00%\n","features.18.proj_conv: weight sparsity=30.00%\n","features.19.start_dw_conv: weight sparsity=29.99%\n","features.19.expand_conv: weight sparsity=30.00%\n","features.19.middle_dw_conv: weight sparsity=30.00%\n","features.19.proj_conv: weight sparsity=30.00%\n","features.20.start_dw_conv: weight sparsity=30.00%\n","features.20.expand_conv: weight sparsity=30.00%\n","features.20.middle_dw_conv: weight sparsity=30.00%\n","features.20.proj_conv: weight sparsity=30.00%\n","features.21.expand_conv: weight sparsity=30.00%\n","features.21.proj_conv: weight sparsity=30.00%\n","features.22.expand_conv: weight sparsity=30.00%\n","features.22.proj_conv: weight sparsity=30.00%\n","features.23.start_dw_conv: weight sparsity=30.00%\n","features.23.expand_conv: weight sparsity=30.00%\n","features.23.proj_conv: weight sparsity=30.00%\n","features.24.block.0: weight sparsity=30.00%\n","conv.block.0: weight sparsity=30.00%\n","classifier: weight sparsity=30.00%\n","Total zeros: 2510748 out of 8369168 (30.00%)\n"]}]},{"cell_type":"code","source":["for name, module in prunned_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        if hasattr(module, 'weight_mask'):\n","            prune.remove(module, 'weight')\n","prunned_model.eval()\n","\n","total_loss = 0.0\n","correct_predictions = 0\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc='Test Progress', leave=False):\n","        inputs = normalize(inputs).to(cpu_device)\n","        labels = labels.to(cpu_device)\n","\n","        outputs = prunned_model(inputs)\n","        loss = CE(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","        total_loss += loss.item() * inputs.size(0)\n","        correct_predictions += (preds == labels).sum().item()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(preds.cpu().numpy())\n","\n","avg_test_loss = total_loss / len(test_loader.dataset)\n","avg_test_acc = accuracy_score(all_labels, all_preds)\n","test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","print(f\"Test Metrics: \"\n","      f\"Loss={avg_test_loss:.4f}, \"\n","      f\"Acc={avg_test_acc:.4f}, \"\n","      f\"Precision={test_precision:.4f}, \"\n","      f\"Recall={test_recall:.4f}, \"\n","      f\"F1={test_f1:.4f}\")\n","\n","print_model_size(prunned_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMavUhJ49Tpn","executionInfo":{"status":"ok","timestamp":1736497339269,"user_tz":-540,"elapsed":5053,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"df4514e9-ddf3-4387-f0fa-64f58d929b23"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Test Metrics: Loss=0.6176, Acc=0.5000, Precision=0.7500, Recall=0.5000, F1=0.3333\n","35.13 MB\n"]}]},{"cell_type":"markdown","source":["# **양자화 인지 훈련 (Quantization Aware Training)**"],"metadata":{"id":"qOZkUtgk9-Fa"}},{"cell_type":"code","source":["num_epochs = 50\n","patience = 5\n","best_val_acc = 0.0\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","early_stop = False\n","\n","QAT_model = copy.deepcopy(ori_model)\n","\n","for params in QAT_model.parameters():\n","    params.requires_grad = True\n","\n","for name, module in QAT_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        prune.l1_unstructured(module, name='weight', amount=0.3)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","QAT_model.to(device)\n","\n","optimizer = optim.Adam(QAT_model.parameters(), lr=0.001)\n","\n","def train_val_epoch(model, dataloader, phase, optimizer=None):\n","    if phase == 'train':\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in tqdm(dataloader, desc=f'  {phase} Progress', leave=False):\n","        inputs = normalize(inputs).to(device)\n","        labels = labels.to(device)\n","\n","        if phase == 'train':\n","            optimizer.zero_grad()\n","\n","        with torch.set_grad_enabled(phase == 'train'):\n","            outputs = model(inputs)\n","            loss = CE(outputs, labels)\n","            _, preds = torch.max(outputs, 1)\n","\n","            if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        all_preds.extend(preds.detach().cpu().numpy())\n","        all_labels.extend(labels.detach().cpu().numpy())\n","\n","    epoch_loss = running_loss / len(dataloader.dataset)\n","    epoch_acc = accuracy_score(all_labels, all_preds)\n","    epoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","    epoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","    epoch_metrics = {\n","        'loss': epoch_loss,\n","        'accuracy': epoch_acc,\n","        'precision': epoch_precision,\n","        'recall': epoch_recall,\n","        'f1': epoch_f1\n","    }\n","\n","    return epoch_loss, epoch_acc, epoch_metrics\n","\n","best_model_wts = copy.deepcopy(QAT_model.state_dict())\n","\n","for epoch in range(num_epochs):\n","    if early_stop:\n","        print(f'Early stopping at epoch {epoch + 1}')\n","        break\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n","\n","    train_loss, train_acc, train_metrics = train_val_epoch(QAT_model, train_loader, 'train', optimizer)\n","\n","    val_loss, val_acc, val_metrics = train_val_epoch(QAT_model, val_loader, 'val')\n","\n","    print(f\"  Train Metrics:      Loss={train_loss:.4f}, Acc={train_acc:.4f}, \"\n","          f\"Precision={train_metrics['precision']:.4f}, \"\n","          f\"Recall={train_metrics['recall']:.4f}, \"\n","          f\"F1={train_metrics['f1']:.4f}\")\n","    print(f\"  Validation Metrics: Loss={val_loss:.4f}, Acc={val_acc:.4f}, \"\n","          f\"Precision={val_metrics['precision']:.4f}, \"\n","          f\"Recall={val_metrics['recall']:.4f}, \"\n","          f\"F1={val_metrics['f1']:.4f}\")\n","\n","    # 모델 저장 및 조기 종료 조건\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_val_acc = val_acc\n","        best_model_wts = copy.deepcopy(QAT_model.state_dict())\n","        epochs_no_improve = 0\n","    else:\n","        epochs_no_improve += 1\n","        print(f\"  Patience: {epochs_no_improve}\")\n","        if epochs_no_improve >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            early_stop = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHzQ8FHotspv","executionInfo":{"status":"ok","timestamp":1736497577868,"user_tz":-540,"elapsed":238602,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"43c4a5f1-b47f-4355-f54d-45a3170bc8ee"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.5699, Acc=0.8857, Precision=0.9070, Recall=0.8857, F1=0.8842\n","  Validation Metrics: Loss=0.8031, Acc=0.5000, Precision=0.7500, Recall=0.5000, F1=0.3333\n","Epoch 2/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.1756, Acc=0.9143, Precision=0.9268, Recall=0.9143, F1=0.9137\n","  Validation Metrics: Loss=0.5276, Acc=0.6000, Precision=0.7778, Recall=0.6000, F1=0.5238\n","Epoch 3/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.1063, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.3603, Acc=0.7000, Precision=0.8125, Recall=0.7000, F1=0.6703\n","Epoch 4/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0713, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.2524, Acc=0.9000, Precision=0.9167, Recall=0.9000, F1=0.8990\n","Epoch 5/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0411, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.1526, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 6/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0300, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.1225, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 7/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0641, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0931, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 8/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0198, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0548, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 9/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0194, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0379, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 10/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0624, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0377, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 11/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0152, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0257, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 12/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0124, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0239, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 13/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0201, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0176, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 14/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0061, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0172, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 15/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0070, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0140, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 16/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0050, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0158, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 17/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0237, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0246, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 2\n","Epoch 18/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0039, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0240, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 3\n","Epoch 19/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0032, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0285, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 4\n","Epoch 20/50:\n"]},{"output_type":"stream","name":"stderr","text":["                                                             "]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0043, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0206, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 5\n","Early stopping at epoch 20\n","Early stopping at epoch 21\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["QAT_model.load_state_dict(best_model_wts)\n","for name, module in QAT_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        if hasattr(module, 'weight_mask'):\n","            prune.remove(module, 'weight')\n","\n","QAT_model.to(cpu_device)\n","int_model = torch.ao.quantization.convert(QAT_model.eval(), inplace=False)\n","int8_path = '/content/drive/MyDrive/Colab Notebooks/hemp/model/int8.pth'\n","torch.save(int_model.state_dict(), int8_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":577},"id":"s6ALZ6LhLQ7m","executionInfo":{"status":"error","timestamp":1736497598446,"user_tz":-540,"elapsed":334,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"7fafeaac-6d0f-46a2-d9fb-50043d31c0c5"},"execution_count":12,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for QuantizableMobileNetV4:\n\tMissing key(s) in state_dict: \"features.0.block.0.weight\", \"features.1.block.0.weight\", \"features.2.block.0.weight\", \"features.3.start_dw_conv.weight\", \"features.3.expand_conv.weight\", \"features.3.middle_dw_conv.weight\", \"features.3.proj_conv.weight\", \"features.4.start_dw_conv.weight\", \"features.4.expand_conv.weight\", \"features.4.middle_dw_conv.weight\", \"features.4.proj_conv.weight\", \"features.5.start_dw_conv.weight\", \"features.5.expand_conv.weight\", \"features.5.middle_dw_conv.weight\", \"features.5.proj_conv.weight\", \"features.6.start_dw_conv.weight\", \"features.6.expand_conv.weight\", \"features.6.middle_dw_conv.weight\", \"features.6.proj_conv.weight\", \"features.7.start_dw_conv.weight\", \"features.7.expand_conv.weight\", \"features.7.middle_dw_conv.weight\", \"features.7.proj_conv.weight\", \"features.8.start_dw_conv.weight\", \"features.8.expand_conv.weight\", \"features.8.middle_dw_conv.weight\", \"features.8.proj_conv.weight\", \"features.9.start_dw_conv.weight\", \"features.9.expand_conv.weight\", \"features.9.middle_dw_conv.weight\", \"features.9.proj_conv.weight\", \"features.10.start_dw_conv.weight\", \"features.10.expand_conv.weight\", \"features.10.proj_conv.weight\", \"features.11.expand_conv.weight\", \"features.11.proj_conv.weight\", \"features.12.start_dw_conv.weight\", \"features.12.expand_conv.weight\", \"features.12.proj_conv.weight\", \"features.13.start_dw_conv.weight\", \"features.13.expand_conv.weight\", \"features.13.middle_dw_conv.weight\", \"features.13.proj_conv.weight\", \"features.14.start_dw_conv.weight\", \"features.14.expand_conv.weight\", \"features.14.middle_dw_conv.weight\", \"features.14.proj_conv.weight\", \"features.15.start_dw_conv.weight\", \"features.15.expand_conv.weight\", \"features.15.middle_dw_conv.weight\", \"features.15.proj_conv.weight\", \"features.16.start_dw_conv.weight\", \"features.16.expand_conv.weight\", \"features.16.middle_dw_conv.weight\", \"features.16.proj_conv.weight\", \"features.17.expand_conv.weight\", \"features.17.proj_conv.weight\", \"features.18.start_dw_conv.weight\", \"features.18.expand_conv.weight\", \"features.18.proj_conv.weight\", \"features.19.start_dw_conv.weight\", \"features.19.expand_conv.weight\", \"features.19.middle_dw_conv.weight\", \"features.19.proj_conv.weight\", \"features.20.start_dw_conv.weight\", \"features.20.expand_conv.weight\", \"features.20.middle_dw_conv.weight\", \"features.20.proj_conv.weight\", \"features.21.expand_conv.weight\", \"features.21.proj_conv.weight\", \"features.22.expand_conv.weight\", \"features.22.proj_conv.weight\", \"features.23.start_dw_conv.weight\", \"features.23.expand_conv.weight\", \"features.23.proj_conv.weight\", \"features.24.block.0.weight\", \"conv.block.0.weight\", \"classifier.weight\". \n\tUnexpected key(s) in state_dict: \"features.0.block.0.weight_orig\", \"features.0.block.0.weight_mask\", \"features.1.block.0.weight_orig\", \"features.1.block.0.weight_mask\", \"features.2.block.0.weight_orig\", \"features.2.block.0.weight_mask\", \"features.3.start_dw_conv.weight_orig\", \"features.3.start_dw_conv.weight_mask\", \"features.3.expand_conv.weight_orig\", \"features.3.expand_conv.weight_mask\", \"features.3.middle_dw_conv.weight_orig\", \"features.3.middle_dw_conv.weight_mask\", \"features.3.proj_conv.weight_orig\", \"features.3.proj_conv.weight_mask\", \"features.4.start_dw_conv.weight_orig\", \"features.4.start_dw_conv.weight_mask\", \"features.4.expand_conv.weight_orig\", \"features.4.expand_conv.weight_mask\", \"features.4.middle_dw_conv.weight_orig\", \"features.4.middle_dw_conv.weight_mask\", \"features.4.proj_conv.weight_orig\", \"features.4.proj_conv.weight_mask\", \"features.5.start_dw_conv.weight_orig\", \"features.5.start_dw_conv.weight_mask\", \"features.5.expand_conv.weight_orig\", \"features.5.expand_conv.weight_mask\", \"features.5.middle_dw_conv.weight_orig\", \"features.5.middle_dw_conv.weight_mask\", \"features.5.proj_conv.weight_orig\", \"features.5.proj_conv.weight_mask\", \"features.6.start_dw_conv.weight_orig\", \"features.6.start_dw_conv.weight_mask\", \"features.6.expand_conv.weight_orig\", \"features.6.expand_conv.weight_mask\", \"features.6.middle_dw_conv.weight_orig\", \"features.6.middle_dw_conv.weight_mask\", \"features.6.proj_conv.weight_orig\", \"features.6.proj_conv.weight_mask\", \"features.7.start_dw_conv.weight_orig\", \"features.7.start_dw_conv.weight_mask\", \"features.7.expand_conv.weight_orig\", \"features.7.expand_conv.weight_mask\", \"features.7.middle_dw_conv.weight_orig\", \"features.7.middle_dw_conv.weight_mask\", \"features.7.proj_conv.weight_orig\", \"features.7.proj_conv.weight_mask\", \"features.8.start_dw_conv.weight_orig\", \"features.8.start_dw_conv.weight_mask\", \"features.8.expand_conv.weight_orig\", \"features.8.expand_conv.weight_mask\", \"features.8.middle_dw_conv.weight_orig\", \"features.8.middle_dw_conv.weight_mask\", \"features.8.proj_conv.weight_orig\", \"features.8.proj_conv.weight_mask\", \"features.9.start_dw_conv.weight_orig\", \"features.9.start_dw_conv.weight_mask\", \"features.9.expand_conv.weight_orig\", \"features.9.expand_conv.weight_mask\", \"features.9.middle_dw_conv.weight_orig\", \"features.9.middle_dw_conv.weight_mask\", \"features.9.proj_conv.weight_orig\", \"features.9.proj_conv.weight_mask\", \"features.10.start_dw_conv.weight_orig\", \"features.10.start_dw_conv.weight_mask\", \"features.10.expand_conv.weight_orig\", \"features.10.expand_conv.weight_mask\", \"features.10.proj_conv.weight_orig\", \"features.10.proj_conv.weight_mask\", \"features.11.expand_conv.weight_orig\", \"features.11.expand_conv.weight_mask\", \"features.11.proj_conv.weight_orig\", \"features.11.proj_conv.weight_mask\", \"features.12.start_dw_conv.weight_orig\", \"features.12.start_dw_conv.weight_mask\", \"features.12.expand_conv.weight_orig\", \"features.12.expand_conv.weight_mask\", \"features.12.proj_conv.weight_orig\", \"features.12.proj_conv.weight_mask\", \"features.13.start_dw_conv.weight_orig\", \"features.13.start_dw_conv.weight_mask\", \"features.13.expand_conv.weight_orig\", \"features.13.expand_conv.weight_mask\", \"features.13.middle_dw_conv.weight_orig\", \"features.13.middle_dw_conv.weight_mask\", \"features.13.proj_conv.weight_orig\", \"features.13.proj_conv.weight_mask\", \"features.14.start_dw_conv.weight_orig\", \"features.14.start_dw_conv.weight_mask\", \"features.14.expand_conv.weight_orig\", \"features.14.expand_conv.weight_mask\", \"features.14.middle_dw_conv.weight_orig\", \"features.14.middle_dw_conv.weight_mask\", \"features.14.proj_conv.weight_orig\", \"features.14.proj_conv.weight_mask\", \"features.15.start_dw_conv.weight_orig\", \"features.15.start_dw_conv.weight_mask\", \"features.15.expand_conv.weight_orig\", \"features.15.expand_conv.weight_mask\", \"features.15.middle_dw_conv.weight_orig\", \"features.15.middle_dw_conv.weight_mask\", \"features.15.proj_conv.weight_orig\", \"features.15.proj_conv.weight_mask\", \"features.16.start_dw_conv.weight_orig\", \"features.16.start_dw_conv.weight_mask\", \"features.16.expand_conv.weight_orig\", \"features.16.expand_conv.weight_mask\", \"features.16.middle_dw_conv.weight_orig\", \"features.16.middle_dw_conv.weight_mask\", \"features.16.proj_conv.weight_orig\", \"features.16.proj_conv.weight_mask\", \"features.17.expand_conv.weight_orig\", \"features.17.expand_conv.weight_mask\", \"features.17.proj_conv.weight_orig\", \"features.17.proj_conv.weight_mask\", \"features.18.start_dw_conv.weight_orig\", \"features.18.start_dw_conv.weight_mask\", \"features.18.expand_conv.weight_orig\", \"features.18.expand_conv.weight_mask\", \"features.18.proj_conv.weight_orig\", \"features.18.proj_conv.weight_mask\", \"features.19.start_dw_conv.weight_orig\", \"features.19.start_dw_conv.weight_mask\", \"features.19.expand_conv.weight_orig\", \"features.19.expand_conv.weight_mask\", \"features.19.middle_dw_conv.weight_orig\", \"features.19.middle_dw_conv.weight_mask\", \"features.19.proj_conv.weight_orig\", \"features.19.proj_conv.weight_mask\", \"features.20.start_dw_conv.weight_orig\", \"features.20.start_dw_conv.weight_mask\", \"features.20.expand_conv.weight_orig\", \"features.20.expand_conv.weight_mask\", \"features.20.middle_dw_conv.weight_orig\", \"features.20.middle_dw_conv.weight_mask\", \"features.20.proj_conv.weight_orig\", \"features.20.proj_conv.weight_mask\", \"features.21.expand_conv.weight_orig\", \"features.21.expand_conv.weight_mask\", \"features.21.proj_conv.weight_orig\", \"features.21.proj_conv.weight_mask\", \"features.22.expand_conv.weight_orig\", \"features.22.expand_conv.weight_mask\", \"features.22.proj_conv.weight_orig\", \"features.22.proj_conv.weight_mask\", \"features.23.start_dw_conv.weight_orig\", \"features.23.start_dw_conv.weight_mask\", \"features.23.expand_conv.weight_orig\", \"features.23.expand_conv.weight_mask\", \"features.23.proj_conv.weight_orig\", \"features.23.proj_conv.weight_mask\", \"features.24.block.0.weight_orig\", \"features.24.block.0.weight_mask\", \"conv.block.0.weight_orig\", \"conv.block.0.weight_mask\", \"classifier.weight_orig\", \"classifier.weight_mask\". ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-34e3aee65717>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQAT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_wts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQAT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QuantizableMobileNetV4:\n\tMissing key(s) in state_dict: \"features.0.block.0.weight\", \"features.1.block.0.weight\", \"features.2.block.0.weight\", \"features.3.start_dw_conv.weight\", \"features.3.expand_conv.weight\", \"features.3.middle_dw_conv.weight\", \"features.3.proj_conv.weight\", \"features.4.start_dw_conv.weight\", \"features.4.expand_conv.weight\", \"features.4.middle_dw_conv.weight\", \"features.4.proj_conv.weight\", \"features.5.start_dw_conv.weight\", \"features.5.expand_conv.weight\", \"features.5.middle_dw_conv.weight\", \"features.5.proj_conv.weight\", \"features.6.start_dw_conv.weight\", \"features.6.expand_conv.weight\", \"features.6.middle_dw_conv.weight\", \"features.6.proj_conv.weight\", \"features.7.start_dw_conv.weight\", \"features.7.expand_conv.weight\", \"features.7.middle_dw_conv.weight\", \"features.7.proj_conv.weight\", \"features.8.start_dw_conv.weight\", \"features.8.expand_conv.weight\", \"features.8.middle_dw_conv.weight\", \"features.8.proj_conv.weight\", \"features.9.start_dw_conv.weight\", \"features.9.expand_conv.weight\", \"features.9.middle_dw_conv.weight\", \"features.9.proj_conv.weight\", \"features.10.start_dw_conv.weight\", \"features.10.expand_conv.weight\", \"features.10.proj_conv.weight\", \"features.11.expand_conv.weight\", \"features.11.proj_conv.weight\", \"features.12.start_dw_conv.weight\", \"features.12.expand_conv.weight\", \"features.12.proj_conv.weight\", \"features.13.start_dw_conv.weight\", \"features.13.expand_conv.weight\", \"features.13.middle_dw_conv.weight\", \"features.13.proj_conv.weight\", \"features.14.start_dw_con...\n\tUnexpected key(s) in state_dict: \"features.0.block.0.weight_orig\", \"features.0.block.0.weight_mask\", \"features.1.block.0.weight_orig\", \"features.1.block.0.weight_mask\", \"features.2.block.0.weight_orig\", \"features.2.block.0.weight_mask\", \"features.3.start_dw_conv.weight_orig\", \"features.3.start_dw_conv.weight_mask\", \"features.3.expand_conv.weight_orig\", \"features.3.expand_conv.weight_mask\", \"features.3.middle_dw_conv.weight_orig\", \"features.3.middle_dw_conv.weight_mask\", \"features.3.proj_conv.weight_orig\", \"features.3.proj_conv.weight_mask\", \"features.4.start_dw_conv.weight_orig\", \"features.4.start_dw_conv.weight_mask\", \"features.4.expand_conv.weight_orig\", \"features.4.expand_conv.weight_mask\", \"features.4.middle_dw_conv.weight_orig\", \"features.4.middle_dw_conv.weight_mask\", \"features.4.proj_conv.weight_orig\", \"features.4.proj_conv.weight_mask\", \"features.5.start_dw_conv.weight_orig\", \"features.5.start_dw_conv.weight_mask\", \"features.5.expand_conv.weight_orig\", \"features.5.expand_conv.weight_mask\", \"features.5.middle_dw_conv.weight_orig\", \"features.5.middle_dw_conv.weight_mask\", \"features.5.proj_conv.weight_orig\", \"features.5.proj_conv.weight_mask\", \"features.6.start_dw_conv.weight_orig\", \"features.6.start_dw_conv.weight_mask\", \"features.6.expand_conv.weight_orig\", \"features.6.expand_conv.weight_mask\", \"features.6.middle_dw_conv.weight_orig\", \"features.6.middle_dw_conv.weight_mask\", \"features.6.proj_conv.weight_orig\", \"features.6.proj_conv.weight_mask\", \"features.7.start_d..."]}]},{"cell_type":"markdown","source":["# **양자화 모델 추론**"],"metadata":{"id":"yjtlubmsMFu_"}},{"cell_type":"code","source":["int_model = copy.deepcopy(ori_model)\n","int_model.to(cpu_device)\n","int_model = torch.ao.quantization.convert(int_model.eval(), inplace=False)\n","int_model.load_state_dict(torch.load(int8_path))\n","int_model.eval()\n","\n","total_loss = 0.0\n","correct_predictions = 0\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc='Test Progress', leave=False):\n","        inputs = normalize(inputs).to(cpu_device)\n","        labels = labels.to(cpu_device)\n","\n","        outputs = int_model(inputs)\n","        loss = CE(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","        total_loss += loss.item() * inputs.size(0)\n","        correct_predictions += (preds == labels).sum().item()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(preds.cpu().numpy())\n","\n","avg_test_loss = total_loss / len(test_loader.dataset)\n","avg_test_acc = accuracy_score(all_labels, all_preds)\n","test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","print(f\"Test Metrics: \"\n","      f\"Loss={avg_test_loss:.4f}, \"\n","      f\"Acc={avg_test_acc:.4f}, \"\n","      f\"Precision={test_precision:.4f}, \"\n","      f\"Recall={test_recall:.4f}, \"\n","      f\"F1={test_f1:.4f}\")\n","\n","print_model_size(int_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bW1uhZPDBIg3","executionInfo":{"status":"ok","timestamp":1736497615212,"user_tz":-540,"elapsed":4307,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"9b7d2464-0cec-4c36-d41d-2835caeff45a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n","  warnings.warn(\n","<ipython-input-14-10d6b952e367>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  int_model.load_state_dict(torch.load(int8_path))\n","                                                            "]},{"output_type":"stream","name":"stdout","text":["Test Metrics: Loss=0.2719, Acc=0.9000, Precision=0.9167, Recall=0.9000, F1=0.8990\n","9.19 MB\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]}]}