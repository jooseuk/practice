{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMJw8998eljeKo7Pk+rIPrW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"7x9tYoPkc0zq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736733130271,"user_tz":-540,"elapsed":3010,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"6461a092-9f45-472a-93e1-f977a4cf79d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# 구글 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys, torch, os, json, copy, csv\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from collections import Counter\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/practice/download/model')\n","from quant import QuantizableMobileNetV4\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch import nn, Tensor\n","from mobilenet import mobilenetv4_conv_medium\n","import torch.nn.utils.prune as prune\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"NWb2yHXftsgF","executionInfo":{"status":"ok","timestamp":1736734149249,"user_tz":-540,"elapsed":336,"user":{"displayName":"전주석","userId":"05296464826010259461"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# **데이터셋 정의**"],"metadata":{"id":"2DbJl1se2Ls1"}},{"cell_type":"code","source":["# 입력 및 라벨 디렉토리 설정\n","input_dir = '/content/drive/MyDrive/Colab Notebooks/practice/download/dataset/seg_images'\n","label_dir = '/content/drive/MyDrive/Colab Notebooks/practice/download/dataset/labels'\n","\n","def load_dataset(input_dir, label_dir):\n","    intput_files = os.listdir(input_dir)\n","    label_files = os.listdir(label_dir)\n","\n","    dataset = []\n","    for input_file in intput_files:\n","        label_file = input_file.replace('.png', '.json')\n","        with open(os.path.join(label_dir, label_file), 'r') as f:\n","            label_data = json.load(f)\n","            browning = label_data['annotations']['polygon'][0]['browning']\n","\n","        dataset.append({\n","            'image_path': os.path.join(input_dir, input_file),\n","            'label': f'{browning}'\n","        })\n","    return dataset\n","\n","dataset = load_dataset(input_dir, label_dir)\n","labels = [data['label'] for data in dataset]\n","\n","# 데이터 분할\n","train_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n","for train_idx, temp_idx in train_split.split(dataset, labels):\n","    train_set = [dataset[i] for i in train_idx]\n","    temp_set = [dataset[i] for i in temp_idx]\n","val_test_split = StratifiedShuffleSplit(n_splits=1, test_size=2/3, random_state=42)\n","labels_temp = [labels[i] for i in temp_idx]\n","for val_idx, test_idx in val_test_split.split(temp_set, labels_temp):\n","    val_set = [temp_set[i] for i in val_idx]\n","    test_set = [temp_set[i] for i in test_idx]\n","\n","# 클래스별 개수 확인\n","train_label_count = Counter([data['label'] for data in train_set])\n","val_label_count   = Counter([data['label'] for data in val_set])\n","test_label_count  = Counter([data['label'] for data in test_set])\n","\n","print(\"Train set class counts:\", train_label_count)\n","print(\"Validation set class counts:\", val_label_count)\n","print(\"Test set class counts:\", test_label_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2btLvgg72CbU","executionInfo":{"status":"ok","timestamp":1736733133197,"user_tz":-540,"elapsed":2398,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"6a37600a-989a-4eac-a52d-9e79c557f098"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set class counts: Counter({'True': 35, 'False': 35})\n","Validation set class counts: Counter({'False': 5, 'True': 5})\n","Test set class counts: Counter({'False': 10, 'True': 10})\n"]}]},{"cell_type":"code","source":["# 전처리 설정\n","MEAN = [0.485, 0.456, 0.406]\n","STD = [0.229, 0.224, 0.225]\n","normalize = transforms.Normalize(mean=MEAN, std=STD)\n","transform = transforms.Compose([\n","    transforms.ToTensor()])\n","\n","def preprocess_image(image):\n","    non_zero_mask = (image > 0).any(axis=-1)\n","    non_zero_rows = np.any(non_zero_mask, axis=1)\n","    non_zero_cols = np.any(non_zero_mask, axis=0)\n","    min_row, max_row = np.where(non_zero_rows)[0][[0, -1]]\n","    min_col, max_col = np.where(non_zero_cols)[0][[0, -1]]\n","    image = image[min_row:max_row+1, min_col:max_col+1]\n","    image = Image.fromarray(image)\n","    image = image.resize((224, 224))\n","    return image\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None, num_classes=2):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","        self.num_classes = num_classes\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n","        image = preprocess_image(image)\n","        label = int(self.labels[idx])\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","def create_dataloader(dataset, transform, batch_size, shuffle, num_workers):\n","    image_paths = [item['image_path'] for item in dataset]\n","    labels = [0 if item['label'] == 'True' else 1 for item in dataset]\n","    custom_dataset = CustomDataset(image_paths, labels, transform=transform)\n","    return DataLoader(custom_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","\n","# 데이터로더 생성\n","train_loader = create_dataloader(train_set, transform, batch_size=24, shuffle=True, num_workers=2)\n","val_loader = create_dataloader(val_set, transform, batch_size=24, shuffle=False, num_workers=2)\n","test_loader = create_dataloader(test_set, transform, batch_size=24, shuffle=False, num_workers=2)"],"metadata":{"id":"uifGGI0F2KDD","executionInfo":{"status":"ok","timestamp":1736733133197,"user_tz":-540,"elapsed":2,"user":{"displayName":"전주석","userId":"05296464826010259461"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# **FP32**"],"metadata":{"id":"TRXs-Z3I5J0i"}},{"cell_type":"code","source":["# 모델 크기 출력 함수\n","def print_model_size(mdl):\n","    torch.save(mdl.state_dict(), \"tmp.pt\")\n","    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n","    os.remove('tmp.pt')\n","\n","# CPU 디바이스 설정 및 손실 함수 정의\n","cpu_device = torch.device(\"cpu\")\n","CE = nn.CrossEntropyLoss()\n","\n","# 사전 학습된 모델 로드 및 평가\n","pretrained_path = '/content/drive/MyDrive/Colab Notebooks/practice/download/model/pretrain.pth'\n","FP32 = mobilenetv4_conv_medium(num_classes=2)\n","FP32.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","FP32.to(cpu_device)\n","FP32.eval()\n","\n","total_loss = 0.0\n","correct_predictions = 0\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc='Test Progress', leave=False):\n","        inputs = normalize(inputs).to(cpu_device)\n","        labels = labels.to(cpu_device)\n","\n","        outputs = FP32(inputs)\n","        loss = CE(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","        total_loss += loss.item() * inputs.size(0)\n","        correct_predictions += (preds == labels).sum().item()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(preds.cpu().numpy())\n","\n","avg_test_loss = total_loss / len(test_loader.dataset)\n","avg_test_acc = accuracy_score(all_labels, all_preds)\n","test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","# 평가지표 출력\n","print(f\"Test Metrics: \"\n","      f\"Loss={avg_test_loss:.4f}, \"\n","      f\"Acc={avg_test_acc:.4f}, \"\n","      f\"Precision={test_precision:.4f}, \"\n","      f\"Recall={test_recall:.4f}, \"\n","      f\"F1={test_f1:.4f}\")\n","\n","# 모델 크기 출력\n","print_model_size(FP32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvUwdLD_5N6q","executionInfo":{"status":"ok","timestamp":1736733142794,"user_tz":-540,"elapsed":8153,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"3e7dda91-24fa-4876-a1ef-4023b557eea5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-7e3ce75e5e98>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  FP32.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n"]},{"output_type":"stream","name":"stdout","text":["Test Metrics: Loss=0.0626, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","34.15 MB\n"]}]},{"cell_type":"markdown","source":["# **비구조적 가지치기 (Unstructured pruning)**"],"metadata":{"id":"zha4k5PR5_vN"}},{"cell_type":"code","source":["# 양자화 가능한 모델 설정\n","model = QuantizableMobileNetV4(num_classes=2)\n","model.to(cpu_device)\n","model.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","model.eval()\n","model.fuse_model()\n","model.train()\n","\n","model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n","torch.ao.quantization.prepare_qat(model, inplace=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoMF0xNF6D2V","executionInfo":{"status":"ok","timestamp":1736733147597,"user_tz":-540,"elapsed":4804,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"a499a61f-fe1a-400c-eb5d-726033598b9d","collapsed":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-4468c621b91a>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(pretrained_path, map_location=cpu_device))\n","/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["QuantizableMobileNetV4(\n","  (features): Sequential(\n","    (0): QuantizableConvBN(\n","      (block): Sequential(\n","        (0): ConvBnReLU2d(\n","          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n","          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","          )\n","          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","          )\n","        )\n","        (1): Identity()\n","        (2): Identity()\n","      )\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (1): QuantizableConvBN(\n","      (block): Sequential(\n","        (0): ConvBnReLU2d(\n","          32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","          )\n","          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","          )\n","        )\n","        (1): Identity()\n","        (2): Identity()\n","      )\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (2): QuantizableConvBN(\n","      (block): Sequential(\n","        (0): ConvBnReLU2d(\n","          128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","          )\n","          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","          )\n","        )\n","        (1): Identity()\n","        (2): Identity()\n","      )\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (3): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False\n","        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False\n","        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        192, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (4): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False\n","        (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (5): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False\n","        (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        480, 480, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=480, bias=False\n","        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (6): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (7): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (8): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        640, 640, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=640, bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (9): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640, bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (10): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (11): QuantizableUniversalInvertedBottleneck(\n","      (expand_conv): ConvBnReLU2d(\n","        160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (12): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        640, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (13): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False\n","        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        960, 960, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=960, bias=False\n","        (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (14): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024, bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (15): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024, bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (16): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024, bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (17): QuantizableUniversalInvertedBottleneck(\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (18): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (19): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512, bias=False\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (20): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (middle_dw_conv): ConvBnReLU2d(\n","        1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1024, bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (middle_dw_norm): Identity()\n","      (middle_dw_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (21): QuantizableUniversalInvertedBottleneck(\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (22): QuantizableUniversalInvertedBottleneck(\n","      (expand_conv): ConvBnReLU2d(\n","        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (23): QuantizableUniversalInvertedBottleneck(\n","      (start_dw_conv): ConvBn2d(\n","        256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256, bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (start_dw_norm): Identity()\n","      (expand_conv): ConvBnReLU2d(\n","        256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (expand_norm): Identity()\n","      (expand_act): Identity()\n","      (proj_conv): ConvBn2d(\n","        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (proj_norm): Identity()\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","    (24): QuantizableConvBN(\n","      (block): Sequential(\n","        (0): ConvBnReLU2d(\n","          256, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","          )\n","          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","          )\n","        )\n","        (1): Identity()\n","        (2): Identity()\n","      )\n","      (float_functional): FloatFunctional(\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (conv): QuantizableConvBN(\n","    (block): Sequential(\n","      (0): ConvBnReLU2d(\n","        960, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n","        (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","        )\n","        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","        )\n","      )\n","      (1): Identity()\n","      (2): Identity()\n","    )\n","    (float_functional): FloatFunctional(\n","      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","      )\n","    )\n","  )\n","  (classifier): Linear(\n","    in_features=1280, out_features=2, bias=True\n","    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n","      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n","      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n","    )\n","    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","    )\n","  )\n","  (quant): QuantStub(\n","    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n","      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n","      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n","    )\n","  )\n","  (dequant): DeQuantStub()\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 모델 복사 및 가지치기\n","ori_model = copy.deepcopy(model)\n","prunned_model = copy.deepcopy(ori_model)\n","for name, module in prunned_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        prune.l1_unstructured(module, name='weight', amount=0.3)\n","\n","# 희소성 계산 함수\n","def count_sparsity(model):\n","    total_params = 0\n","    total_zero = 0\n","    for name, module in model.named_modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            weight_sparsity = float(torch.sum(module.weight == 0)) / module.weight.numel()\n","            print(f\"{name}: weight sparsity={weight_sparsity*100:.2f}%\")\n","            total_zero += torch.sum(module.weight == 0).item()\n","            total_params += module.weight.numel()\n","    print(f\"Total zeros: {total_zero} out of {total_params} ({(total_zero / total_params) * 100:.2f}%)\")\n","\n","# 희소성 출력\n","count_sparsity(prunned_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Daotx5r08SqQ","executionInfo":{"status":"ok","timestamp":1736733181054,"user_tz":-540,"elapsed":1372,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"000b3eb8-7dca-4689-86b1-8e3021a0f6db"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["features.0.block.0: weight sparsity=29.98%\n","features.1.block.0: weight sparsity=30.00%\n","features.2.block.0: weight sparsity=30.00%\n","features.3.start_dw_conv: weight sparsity=30.09%\n","features.3.expand_conv: weight sparsity=30.00%\n","features.3.middle_dw_conv: weight sparsity=30.00%\n","features.3.proj_conv: weight sparsity=30.00%\n","features.4.start_dw_conv: weight sparsity=30.00%\n","features.4.expand_conv: weight sparsity=30.00%\n","features.4.middle_dw_conv: weight sparsity=30.00%\n","features.4.proj_conv: weight sparsity=30.00%\n","features.5.start_dw_conv: weight sparsity=30.00%\n","features.5.expand_conv: weight sparsity=30.00%\n","features.5.middle_dw_conv: weight sparsity=30.00%\n","features.5.proj_conv: weight sparsity=30.00%\n","features.6.start_dw_conv: weight sparsity=30.00%\n","features.6.expand_conv: weight sparsity=30.00%\n","features.6.middle_dw_conv: weight sparsity=30.00%\n","features.6.proj_conv: weight sparsity=30.00%\n","features.7.start_dw_conv: weight sparsity=30.00%\n","features.7.expand_conv: weight sparsity=30.00%\n","features.7.middle_dw_conv: weight sparsity=30.00%\n","features.7.proj_conv: weight sparsity=30.00%\n","features.8.start_dw_conv: weight sparsity=30.00%\n","features.8.expand_conv: weight sparsity=30.00%\n","features.8.middle_dw_conv: weight sparsity=30.00%\n","features.8.proj_conv: weight sparsity=30.00%\n","features.9.start_dw_conv: weight sparsity=30.00%\n","features.9.expand_conv: weight sparsity=30.00%\n","features.9.middle_dw_conv: weight sparsity=30.00%\n","features.9.proj_conv: weight sparsity=30.00%\n","features.10.start_dw_conv: weight sparsity=30.00%\n","features.10.expand_conv: weight sparsity=30.00%\n","features.10.proj_conv: weight sparsity=30.00%\n","features.11.expand_conv: weight sparsity=30.00%\n","features.11.proj_conv: weight sparsity=30.00%\n","features.12.start_dw_conv: weight sparsity=30.00%\n","features.12.expand_conv: weight sparsity=30.00%\n","features.12.proj_conv: weight sparsity=30.00%\n","features.13.start_dw_conv: weight sparsity=30.00%\n","features.13.expand_conv: weight sparsity=30.00%\n","features.13.middle_dw_conv: weight sparsity=30.00%\n","features.13.proj_conv: weight sparsity=30.00%\n","features.14.start_dw_conv: weight sparsity=30.00%\n","features.14.expand_conv: weight sparsity=30.00%\n","features.14.middle_dw_conv: weight sparsity=30.00%\n","features.14.proj_conv: weight sparsity=30.00%\n","features.15.start_dw_conv: weight sparsity=29.99%\n","features.15.expand_conv: weight sparsity=30.00%\n","features.15.middle_dw_conv: weight sparsity=30.00%\n","features.15.proj_conv: weight sparsity=30.00%\n","features.16.start_dw_conv: weight sparsity=29.99%\n","features.16.expand_conv: weight sparsity=30.00%\n","features.16.middle_dw_conv: weight sparsity=30.00%\n","features.16.proj_conv: weight sparsity=30.00%\n","features.17.expand_conv: weight sparsity=30.00%\n","features.17.proj_conv: weight sparsity=30.00%\n","features.18.start_dw_conv: weight sparsity=29.99%\n","features.18.expand_conv: weight sparsity=30.00%\n","features.18.proj_conv: weight sparsity=30.00%\n","features.19.start_dw_conv: weight sparsity=29.99%\n","features.19.expand_conv: weight sparsity=30.00%\n","features.19.middle_dw_conv: weight sparsity=30.00%\n","features.19.proj_conv: weight sparsity=30.00%\n","features.20.start_dw_conv: weight sparsity=30.00%\n","features.20.expand_conv: weight sparsity=30.00%\n","features.20.middle_dw_conv: weight sparsity=30.00%\n","features.20.proj_conv: weight sparsity=30.00%\n","features.21.expand_conv: weight sparsity=30.00%\n","features.21.proj_conv: weight sparsity=30.00%\n","features.22.expand_conv: weight sparsity=30.00%\n","features.22.proj_conv: weight sparsity=30.00%\n","features.23.start_dw_conv: weight sparsity=30.00%\n","features.23.expand_conv: weight sparsity=30.00%\n","features.23.proj_conv: weight sparsity=30.00%\n","features.24.block.0: weight sparsity=30.00%\n","conv.block.0: weight sparsity=30.00%\n","classifier: weight sparsity=30.00%\n","Total zeros: 2510748 out of 8369168 (30.00%)\n"]}]},{"cell_type":"code","source":["# 가지치기 마스크 제거\n","for name, module in prunned_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        if hasattr(module, 'weight_mask'):\n","            prune.remove(module, 'weight')\n","prunned_model.eval()\n","\n","# 가지치기된 모델 평가\n","total_loss = 0.0\n","correct_predictions = 0\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc='Test Progress', leave=False):\n","        inputs = normalize(inputs).to(cpu_device)\n","        labels = labels.to(cpu_device)\n","\n","        outputs = prunned_model(inputs)\n","        loss = CE(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","        total_loss += loss.item() * inputs.size(0)\n","        correct_predictions += (preds == labels).sum().item()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(preds.cpu().numpy())\n","\n","avg_test_loss = total_loss / len(test_loader.dataset)\n","avg_test_acc = accuracy_score(all_labels, all_preds)\n","test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","print(f\"Test Metrics: \"\n","      f\"Loss={avg_test_loss:.4f}, \"\n","      f\"Acc={avg_test_acc:.4f}, \"\n","      f\"Precision={test_precision:.4f}, \"\n","      f\"Recall={test_recall:.4f}, \"\n","      f\"F1={test_f1:.4f}\")\n","\n","# 가지치기된 모델 크기 출력\n","print_model_size(prunned_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMavUhJ49Tpn","executionInfo":{"status":"ok","timestamp":1736733215406,"user_tz":-540,"elapsed":7120,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"95f65db5-4407-4aa5-8381-cad79ed73366"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["                                                            "]},{"output_type":"stream","name":"stdout","text":["Test Metrics: Loss=0.6503, Acc=0.5000, Precision=0.7500, Recall=0.5000, F1=0.3333\n","35.13 MB\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"markdown","source":["# **양자화 인지 훈련 (Quantization Aware Training)**"],"metadata":{"id":"qOZkUtgk9-Fa"}},{"cell_type":"code","source":["# 로그 경로\n","log_dir = '/content/drive/MyDrive/Colab Notebooks/practice/log'\n","log_csv_path = os.path.join(log_dir, 'QAT_log.csv')\n","\n","# 하이퍼파라미터 설정\n","num_epochs = 50\n","patience = 5\n","best_val_acc = 0.0\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","early_stop = False\n","\n","# QAT 모델 복사\n","QAT_model = copy.deepcopy(ori_model)\n","for params in QAT_model.parameters():\n","    params.requires_grad = True\n","\n","# 가지치기 적용\n","for name, module in QAT_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        prune.l1_unstructured(module, name='weight', amount=0.3)\n","\n","# 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","QAT_model.to(device)\n","\n","# 옵티마이저 설정\n","optimizer = optim.Adam(QAT_model.parameters(), lr=0.001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3m2eyxCSbM6a","executionInfo":{"status":"ok","timestamp":1736734203754,"user_tz":-540,"elapsed":2640,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"a45eeafe-4d6b-417d-bc47-f8ee0999ceae"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["# 모델 학습\n","def train_val_epoch(model, dataloader, phase, optimizer=None):\n","    if phase == 'train':\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in tqdm(dataloader, desc=f'  {phase} Progress', leave=False):\n","        inputs = normalize(inputs).to(device)\n","        labels = labels.to(device)\n","\n","        if phase == 'train':\n","            optimizer.zero_grad()\n","\n","        with torch.set_grad_enabled(phase == 'train'):\n","            outputs = model(inputs)\n","            loss = CE(outputs, labels)\n","            _, preds = torch.max(outputs, 1)\n","\n","            if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        all_preds.extend(preds.detach().cpu().numpy())\n","        all_labels.extend(labels.detach().cpu().numpy())\n","\n","    epoch_loss = running_loss / len(dataloader.dataset)\n","    epoch_acc = accuracy_score(all_labels, all_preds)\n","    epoch_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","    epoch_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","    epoch_metrics = {\n","        'loss': epoch_loss,\n","        'accuracy': epoch_acc,\n","        'precision': epoch_precision,\n","        'recall': epoch_recall,\n","        'f1': epoch_f1\n","    }\n","\n","    return epoch_loss, epoch_acc, epoch_metrics\n","\n","# CSV 파일에 에포크별 로그 작성\n","with open(log_csv_path, 'w', newline='') as csvfile:\n","    fieldnames = [\n","        'epoch',\n","        'train_loss', 'train_acc', 'train_precision', 'train_recall', 'train_f1',\n","        'val_loss', 'val_acc', 'val_precision', 'val_recall', 'val_f1'\n","    ]\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    # 훈련 루프\n","    for epoch in range(num_epochs):\n","        if early_stop:\n","            print(f'Early stopping at epoch {epoch + 1}')\n","            break\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n","\n","        train_loss, train_acc, train_metrics = train_val_epoch(QAT_model, train_loader, 'train', optimizer)\n","        val_loss, val_acc, val_metrics = train_val_epoch(QAT_model, val_loader, 'val')\n","\n","        print(f\"  Train Metrics:      Loss={train_loss:.4f}, Acc={train_acc:.4f}, \"\n","              f\"Precision={train_metrics['precision']:.4f}, \"\n","              f\"Recall={train_metrics['recall']:.4f}, \"\n","              f\"F1={train_metrics['f1']:.4f}\")\n","        print(f\"  Validation Metrics: Loss={val_loss:.4f}, Acc={val_acc:.4f}, \"\n","              f\"Precision={val_metrics['precision']:.4f}, \"\n","              f\"Recall={val_metrics['recall']:.4f}, \"\n","              f\"F1={val_metrics['f1']:.4f}\")\n","\n","        # 현재 에포크의 로그를 CSV 파일에 작성\n","        writer.writerow({\n","            'epoch': epoch + 1,\n","            'train_loss': train_loss,\n","            'train_acc': train_acc,\n","            'train_precision': train_metrics['precision'],\n","            'train_recall': train_metrics['recall'],\n","            'train_f1': train_metrics['f1'],\n","            'val_loss': val_loss,\n","            'val_acc': val_acc,\n","            'val_precision': val_metrics['precision'],\n","            'val_recall': val_metrics['recall'],\n","            'val_f1': val_metrics['f1']\n","        })\n","\n","        # 조기 종료 및 최적 모델 저장\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_val_acc = val_acc\n","            best_model_wts = copy.deepcopy(QAT_model.state_dict())\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","            print(f\"  Patience: {epochs_no_improve}\")\n","            if epochs_no_improve >= patience:\n","                early_stop = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHzQ8FHotspv","executionInfo":{"status":"ok","timestamp":1736734600840,"user_tz":-540,"elapsed":396042,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"cf3d3c0b-ad33-4e98-9bb8-ea13fe13d3a9"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.7286, Acc=0.8143, Precision=0.8487, Recall=0.8143, F1=0.8096\n","  Validation Metrics: Loss=0.5842, Acc=0.5000, Precision=0.7500, Recall=0.5000, F1=0.3333\n","Epoch 2/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.6163, Acc=0.8857, Precision=0.9070, Recall=0.8857, F1=0.8842\n","  Validation Metrics: Loss=0.4200, Acc=0.6000, Precision=0.7778, Recall=0.6000, F1=0.5238\n","Epoch 3/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.1760, Acc=0.9714, Precision=0.9730, Recall=0.9714, F1=0.9714\n","  Validation Metrics: Loss=0.2839, Acc=0.8000, Precision=0.8571, Recall=0.8000, F1=0.7917\n","Epoch 4/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.1046, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.1952, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 5/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0806, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.1004, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 6/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.1093, Acc=0.9714, Precision=0.9730, Recall=0.9714, F1=0.9714\n","  Validation Metrics: Loss=0.0883, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 7/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0671, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.0545, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 8/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0609, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.0523, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 9/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0579, Acc=0.9714, Precision=0.9730, Recall=0.9714, F1=0.9714\n","  Validation Metrics: Loss=0.0403, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 10/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0610, Acc=0.9714, Precision=0.9730, Recall=0.9714, F1=0.9714\n","  Validation Metrics: Loss=0.0529, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 11/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0394, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.0349, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 12/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0373, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.0219, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 13/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0403, Acc=0.9857, Precision=0.9861, Recall=0.9857, F1=0.9857\n","  Validation Metrics: Loss=0.0327, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 14/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0196, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0246, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 2\n","Epoch 15/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0168, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0145, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 16/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0156, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0135, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 17/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0081, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0137, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 18/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0094, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0201, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 2\n","Epoch 19/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0158, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0144, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 3\n","Epoch 20/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0055, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0078, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 21/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0022, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0044, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 22/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0028, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0039, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 23/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0049, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0055, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 24/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0007, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0016, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 25/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0058, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0013, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 26/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0007, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0011, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 27/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0002, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0006, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 28/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0003, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 29/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0002, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0004, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 30/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0010, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0003, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 31/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0003, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 32/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0012, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 33/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0000, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","Epoch 34/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 1\n","Epoch 35/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0005, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 2\n","Epoch 36/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0003, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 3\n","Epoch 37/50:\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0000, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0001, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 4\n","Epoch 38/50:\n"]},{"output_type":"stream","name":"stderr","text":["                                                             "]},{"output_type":"stream","name":"stdout","text":["  Train Metrics:      Loss=0.0000, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Validation Metrics: Loss=0.0003, Acc=1.0000, Precision=1.0000, Recall=1.0000, F1=1.0000\n","  Patience: 5\n","Early stopping at epoch 39\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["# 최적 모델 가중치 로드 및 가지치기 제거\n","QAT_model.load_state_dict(best_model_wts)\n","for name, module in QAT_model.named_modules():\n","    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n","        if hasattr(module, 'weight_mask'):\n","            prune.remove(module, 'weight')\n","\n","# 양자화 변환 및 저장\n","QAT_model.to(cpu_device)\n","int_model = torch.ao.quantization.convert(QAT_model.eval(), inplace=False)\n","int8_path = '/content/drive/MyDrive/Colab Notebooks/practice/model/int8.pth'\n","torch.save(int_model.state_dict(), int8_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6ALZ6LhLQ7m","executionInfo":{"status":"ok","timestamp":1736734601570,"user_tz":-540,"elapsed":732,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"d08a75b6-31f4-4142-8bdf-5c72dc76a252"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 로그 파일 읽기\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/practice/log/QAT_log.csv')\n","\n","epochs = df['epoch']\n","train_acc = df['train_acc']\n","val_acc = df['val_acc']\n","train_loss = df['train_loss']\n","val_loss = df['val_loss']\n","\n","# 정확도 그래프\n","fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n","axes[0].plot(epochs, train_acc, label='Train Accuracy', marker='o', color='orange')\n","axes[0].plot(epochs, val_acc, label='Validation Accuracy', marker='o', color='red')\n","axes[0].set_xlabel('Epochs')\n","axes[0].set_ylabel('Accuracy')\n","axes[0].legend()\n","axes[0].grid()\n","\n","# 손실 그래프\n","axes[1].plot(epochs, train_loss, label='Train Loss', marker='o', color='orange')\n","axes[1].plot(epochs, val_loss, label='Validation Loss', marker='o', color='red')\n","axes[1].set_xlabel('Epochs')\n","axes[1].set_ylabel('Loss')\n","axes[1].legend()\n","axes[1].grid()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"id":"2gt48wT8ZuyK","executionInfo":{"status":"ok","timestamp":1736734602661,"user_tz":-540,"elapsed":1093,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"1f29619f-9e9d-40c7-eefd-f5f0fc57eebc"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1400x600 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# **양자화 모델 추론**"],"metadata":{"id":"yjtlubmsMFu_"}},{"cell_type":"code","source":["# 양자화 모델 로드\n","int_model = copy.deepcopy(ori_model)\n","int_model.to(cpu_device)\n","int_model = torch.ao.quantization.convert(int_model.eval(), inplace=False)\n","int_model.load_state_dict(torch.load(int8_path))\n","int_model.eval()\n","\n","\n","# 양자화된 모델 테스트\n","total_loss = 0.0\n","correct_predictions = 0\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(test_loader, desc='Test Progress', leave=False):\n","        inputs = normalize(inputs).to(cpu_device)\n","        labels = labels.to(cpu_device)\n","\n","        outputs = int_model(inputs)\n","        loss = CE(outputs, labels)\n","        _, preds = torch.max(outputs, 1)\n","\n","        total_loss += loss.item() * inputs.size(0)\n","        correct_predictions += (preds == labels).sum().item()\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(preds.cpu().numpy())\n","\n","avg_test_loss = total_loss / len(test_loader.dataset)\n","avg_test_acc = accuracy_score(all_labels, all_preds)\n","test_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n","test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n","\n","# 테스트 결과 출력\n","print(f\"Test Metrics: \"\n","      f\"Loss={avg_test_loss:.4f}, \"\n","      f\"Acc={avg_test_acc:.4f}, \"\n","      f\"Precision={test_precision:.4f}, \"\n","      f\"Recall={test_recall:.4f}, \"\n","      f\"F1={test_f1:.4f}\")\n","\n","# 양자화 모델 크기 출력\n","print_model_size(int_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bW1uhZPDBIg3","executionInfo":{"status":"ok","timestamp":1736734606683,"user_tz":-540,"elapsed":4025,"user":{"displayName":"전주석","userId":"05296464826010259461"}},"outputId":"4e2f2bf4-ce3d-4fed-9f93-d87af9845ba9"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n","  warnings.warn(\n","<ipython-input-29-645ca30a0f8e>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  int_model.load_state_dict(torch.load(int8_path))\n","                                                            "]},{"output_type":"stream","name":"stdout","text":["Test Metrics: Loss=0.0654, Acc=0.9500, Precision=0.9545, Recall=0.9500, F1=0.9499\n","9.19 MB\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]}]}